{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe goal of this notebook is to classify an image of a handwritten letter into one of 33 categories/letters of the Russian alphabet using deep learning technologies (computer vision).\n\nReferences:\n- The dataset was prepared and uploaded by [Olga Belitskaya](https://www.kaggle.com/olgabelitskaya) including the total of 14190 images.\n\nLetter Symbols => Letter Labels:\n\nа=>1, б=>2, в=>3, г=>4, д=>5, е=>6, ё=>7, ж=>8, з=>9, и=>10,\nй=>11, к=>12, л=>13, м=>14, н=>15, о=>16, п=>17, р=>18, с=>19, т=>20,\nу=>21, ф=>22, х=>23, ц=>24, ч=>25, ш=>26, щ=>27, ъ=>28, ы=>29, ь=>30,\nэ=>31, ю=>32, я=>33\n\n- Most of the explanations are borrowed from the [Kaggle *Deep Learning* course](https://www.kaggle.com/learn/deep-learning) and this [kernel](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6/output#Introduction-to-CNN-Keras---Acc-0.997-(top-8%).\n\nI will develope a Sequential Convolutional Neural Network for this project. I've chosen to build it with keras API (Tensorflow backend) which is very intuitive. Firstly, I will prepare the data (handwritten letters images) then I will focus on the CNN modeling and evaluation.\n\nThis Notebook has four main parts:\n\n1. Data preparation\n2. Model creation and tuning\n3. Model evaluation\n4. Model prediction on test data\n\nThe logbook of this project can be found [here](https://docs.google.com/spreadsheets/d/15L4IlWvsdMmVphFHvqlhz3lmE25VTatBQBejyFZUyK0/edit?usp=sharing).\nIt includes the following tabs:\n* Time spent on the project\n* Ideas to improve the model\n* Logbook of different configurations I tested\n* Lessons learnt from the project\n\nThe **GitHub repository** of this project is [here](https://github.com/TatianaSnauwaert/Deep_RU_letters)."},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preparation\n## 1.1 Load libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the libraries \n\nimport sys\nimport seaborn as sns\nimport numpy as np\nnp.set_printoptions(threshold=sys.maxsize)\nimport pandas as pd\nimport os\nimport h5py\nimport PIL\nimport cv2\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pylab as plt\nfrom matplotlib import cm\n%matplotlib inline\nimport matplotlib.pylab as pylab\npylab.rcParams[\"figure.figsize\"] = (14,8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Load input folder and see an example of input data"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_folder = '/kaggle/input/russian-handwritten-letters/all_letters_image/all_letters_image/'\nall_letters_filename = os.listdir(input_folder)\nlen(all_letters_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"i = Image.open(\"/kaggle/input/russian-handwritten-letters/all_letters_image/all_letters_image/20_102.png\")\ni","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is one of our images. Each image has a size of 32 by 32 pixels. We then convert each image into a 3d numpy array."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"i_arr = np.array(i)\ni_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All 32 matrices inside this array represent one image. \n* Each matrix represents 1 line of this image. \n* One line of the image is 32 pixels long, so each matrix has 32 rows. \n* Each row inside a matrix has 4 columns and it represents 1 pixel. For that pixel each column represents the color values - how red, green and blue it is - plus the opacity of the colors (last column).  \n\nThat is why each matrix is 32 by 4. The total amount of pixels inside one image is 32 * 32 = 1024.\n\nEach color value can be found in the range of [0:255]. It means there are 256 shades for each color. In total all of the combinations of these colors give us 256ˆ3 = 16 777 216 possible colors."},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Convert images to tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions to preprocess an image into a tensor. \n# We will use the default RGB mode \n# instead of a possible RGBA as the opacity doesn't seem to be important in this task\n\n#TO DO: describe the function\n\ndef img_to_array(img_name, input_folder):\n    img = image.load_img(input_folder + img_name, target_size=(32,32))\n    x = image.img_to_array(img)\n    return np.expand_dims(x, axis=0)\ndef data_to_tensor(img_names, input_folder):\n    list_of_tensors = [img_to_array(img_name, input_folder) for img_name in img_names]\n    return np.vstack(list_of_tensors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/russian-handwritten-letters/all_letters_info.csv\")\nimage_names = data['file']\nletters = data[ 'letter']\nbackgrounds = data['background'].values\ntargets = data['label'].values\ntensors = data_to_tensor(image_names, input_folder)\ntensors[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the shape \nprint ('Tensor shape:', tensors.shape)\nprint ('Target shape', targets.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read from files and display images using OpenCV\ndef display_images(img_path, ax):\n    img = cv2.imread(input_folder + img_path)\n    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    \nfig = plt.figure(figsize=(16, 4))\nfor i in range(12):\n    ax = fig.add_subplot(2, 6, i + 1, xticks=[], yticks=[], title=letters[i*100])\n    display_images(image_names[i*100], ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the labels distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.countplot(targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classes are perfectly balanced which is very important for a classification model. If classes are imbalanced the model will try to maximize the accuracy of the majority class while leaving out other classes leading to less accurate predictions for minority classes. "},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Data preprocessing\n### 1.4.1 Normalization\nWe perform a normalization to reduce the effect of illumination's differences.\n\nMoreover a CNN converges faster on [0..1] data than on [0..255].\nWe will transform the input data to the float type and then divide by 255 (maximum brightness for each color). "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tensors.astype(\"float32\")/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = X[0]\narr_ = np.squeeze(arr)\nplt.imshow(arr_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = targets\n\nimg_rows, img_cols = 32, 32 # because our pictures are 32 by 32 pixels\nnum_classes = 33 # because there are 33 letters in the Russina alphabet\n\ny = keras.utils.to_categorical(y-1, num_classes) # targets-1 because our list starts with 1 and not 0 as expected by keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4.2 Letter detection to remove background and other noise\n\nThe below function reference: [an answer to a stockoverflow question.](https://stackoverflow.com/questions/24385714/detect-text-region-in-image-using-opencv)\n\np.s.: In the end this algorithm is not working for this dataset (see below)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def captch_ex(file_name):\n    img = cv2.imread(file_name)\n    img_final = cv2.imread(file_name)\n    img2gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ret, mask = cv2.threshold(img2gray, 180, 255, cv2.THRESH_BINARY)\n    image_final = cv2.bitwise_and(img2gray, img2gray, mask=mask)\n    ret, new_img = cv2.threshold(image_final, 180, 255, cv2.THRESH_BINARY)  # for black text , cv.THRESH_BINARY_INV\n\n    '''\n            line  8 to 12  : Remove noisy portion \n    '''\n    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,\n                                                         3))  # to manipulate the orientation of dilution , large x means horizonatally dilating  more, large y means vertically dilating more\n    dilated = cv2.dilate(new_img, kernel, iterations=9)  # dilate , more the iteration more the dilation\n\n    # for cv2.x.x\n\n    contours, hierarchy = cv2.findContours(new_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # findContours returns 3 variables for getting contours\n\n    for contour in contours:\n        # get rectangle bounding contour\n        [x, y, w, h] = cv2.boundingRect(contour)\n\n        # Don't plot small false positives that aren't text\n        if w < 35 and h < 35:\n            continue\n\n        # draw rectangle around contour on original image\n        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 2)\n\n        '''\n        #you can crop image and send to OCR  , false detected will return no text :)\n        cropped = img_final[y :y +  h , x : x + w]\n\n        s = file_name + '/crop_' + str(index) + '.png' \n        cv2.imwrite(s , cropped)\n        index = index + 1\n\n        '''\n    # write original image with added contours to disk\n    cv2.imshow('captcha_result', img)\n    cv2.waitKey()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = '/kaggle/input/russian-handwritten-letters/all_letters_image/all_letters_image/04_100.png'\n# captch_ex(file_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like this algorithm is unable to identify the contours of a letter as it returns None for this line:\n\ncontours, hierarchy = cv2.findContours(new_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  \n\nAfter careful examination I found that pixel intensities do not differ significantly for a letter and the rest of the image. I suspect this is the reason for the letter detection failure. Maybe try a different algorithm later.\n\nNext I will try to center the letters and crop the images."},{"metadata":{},"cell_type":"markdown","source":"### 1.4.3 Greyscale images\n\nTurning images to grey scale didn't improve the model but even worsen the validation and train scores by a few percent. I decided to convert back to RGB in the final model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grayscaled tensors\nX_grey = np.dot(X[...,:3], [0.299, 0.587, 0.114])\n# X_grey = tf.expand_dims(X_grey, axis=3)\nprint ('Grayscaled Tensor shape:', X_grey.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_grey[0], cmap=plt.get_cmap(\"gray\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Model creation and tuning\n## 2.1 Train_test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into train, validation and test sets.\n\nX_train_whole, X_test, y_train_whole, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_whole, y_train_whole, test_size=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Data augmentation\n\nA very straightforward way to understand why data augmentation works is by thinking of it as a way to artificially expand our dataset. As is the case with deep learning applications, the more data, the merrier.\n\nEach time the neural network sees the same image, it's a bit different due to the stochastic data augmentation being applied to it.  This difference can be seen as noise being added to our data sample each time, and this noise forces the neural network to learn generalised features instead of overfitting on the dataset."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1 Horizontal flipping of the part of dataset\n\nHorizontal flipping is a data augmentation technique. Usually a part of data is randomly flipped horizontally to provide a wider set of images for a model to better learn the patterns. Our dataset contains letters, so not all of them can be flipped. If we flip letters that are not symmetrical, it will confuse the algorithm as a letter won't be recognizable.\n\nBelow I have tried to only flip symmetrical letters. The full list of those include 10 letters: ж=>8, л=>13, м=>14, н=>15, о=>16, п=>17, т=>20, ф=>22, х=>23, ш=>26.\n\nFor that purpose I have subsetted all the lines from the training set that correspond to these 10 letters and flipped these images with the tensorflow function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# flip_labels = [8,13,14,15,16,17,20,22,23,26]\n# flip_labels = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mask = np.isin(y_train, flip_labels)\n# X_train_to_flip = X_train[mask]\n# flipped_y_train = y_train[mask]\n# len(flipped_y_train)\n# flipped_X_train = tf.image.flip_left_right(X_train_to_flip)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# plt.imshow(X_train_to_flip[15])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.imshow(flipped_X_train[15])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aug_X_train = np.concatenate((X_train, flipped_X_train), axis=0)\n# len(aug_X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aug_y_train = np.concatenate((y_train, flipped_y_train), axis=0)\n# len(aug_y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform our labels into categorical data using keras tools: \nit automatically does the one-hot encoding turning the target column into 33 columns for each letter. \n\nIn every column ones for this column's letter and all others are zeros. "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# img_rows, img_cols = 32, 32 # because our pictures are 32 by 32 pixels\n# num_classes = 33 # because there are 33 letters in the Russina alphabet\n# aug_y_train = keras.utils.to_categorical(aug_y_train-1, num_classes) # targets-1 because our list starts with 1 and not 0 as expected by keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After testing this change on the training and validation data I found that the validation score has increased slightly but the validation score has dropped significantly - from 87.8% to 0.35%. I then tested flipping only 1 letter. Turns out the validation score dropped only a few percentile. I made a conclusion that augmenting data only for some classes in the dataset has caused a class imbalance issue and consequently such a dramatic drop in the validation score. The model wasn't generalizing well.\n\nA lot of time was spent on this idea but in the end I have to remove it from the model. \nNote for the future: think through the idea before implementing it - what are the possible outcomes and how much time it might take."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2 Automatic random data augmentation\n\nBelow I will use a built-in function to perform different kind of random data augmentation.\n\nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model."},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample/image mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n# grey_X_train = tf.expand_dims(X_train, axis=3)\n# grey_X_val = tf.expand_dims(X_val, axis=3)\n# grey_X_test = tf.expand_dims(X_test, axis=3)\n\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the data augmentation, I chose to :\n\n* Randomly rotate some training images by 10 degrees;\n* Randomly Zoom by 10% some training images;\n* Randomly shift images horizontally by 10% of the width;\n* Randomly shift images vertically by 10% of the height."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Define the model\n\nFirst we always instantiate the Sequential model. We then add the first Conv2D layer in which we specify the number of filters/convolutions, kernel_size (the shape of the convolution itself - can be an integer (if hight and width are the same) or a tuple). \n\nWe need to specify an activation function as well.\n\nRectified Linear Units (ReLU) is the most commonly used activation function in deep learning models. \n\nf(x)=max(0,x) \n\nActivation functions serve two primary purposes: 1) Help a model account for interaction effects. 2) Help a model account for non-linear effects (the effect of increasing the predictor by one is different at different values of that predictor). \n\n[ReLU kaggle](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)\n\nWe must also specify the input_shape only in the first layer (number of pixel rows, pixel columns and number of color channels). \n\nThe second important layer in CNN is the pooling (MaxPooling2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximum value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area pooled each time):the higher the pooling dimension is, the more the significant the downsampling is.\n\nDropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n\nThen we add as many layers as we like, then add the Flatten layer (reduces the number of dimensions from 4d to 2d),\nat the end we add the Dense layer which connects all the neurons (unlike in the previous layers where not all neurons might be connected) and is basically just an artificial neural networks (ANN) classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model architecture\n\ndeep_RU_model = Sequential()\n\ndeep_RU_model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (img_rows,img_cols,3)))\ndeep_RU_model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\ndeep_RU_model.add(MaxPooling2D(pool_size=(2,2)))\ndeep_RU_model.add(Dropout(0.25))\n\n\ndeep_RU_model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\ndeep_RU_model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\ndeep_RU_model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\ndeep_RU_model.add(Dropout(0.25))\n\n\ndeep_RU_model.add(Flatten())\ndeep_RU_model.add(Dense(256, activation = \"relu\"))\ndeep_RU_model.add(Dropout(0.5))\ndeep_RU_model.add(Dense(33, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Compile the model\nIn this section we define **HOW** the model gets its weights. \nIn other words we configure the model for training. \n\n3 important concepts:\n- loss function\n- gradient descent\n- backward propagation\n\nLoss function measures how good our model's predictions are.\nLoss = f (actual, prediction)\nWe try to minimize the loss, so the closer the prediction is to the actual value, the lower the loss is.\nThe model's loss function number will change if we change the weights. We use \"categorical_crossentropy\" loss function for the multiclass (>2 classes) classification.\n\nThe model finds the best weights through the \"gradient descent\": it takes one step and predicts in which direction it goes downhill the fastest, then it goes that direction one more step and repeats until it can't go down anymore.\n\nTo see which way is downhill - we use a \"backward propagation\".\nFirst consider the weights in the layer before the Dense layer. Since we know in the training data the actual label of that particular image, we slightly increase those weights that lead to the correct label and slightly decease weights that lead to the wrong label. Then we continue the same way to go back until the first layer after the input data.\n\nThe size of weight changing is determined by the learning rate.\nThe optimizer=\"adam\" is the special variation of gradient descent that automatically figures out the best learning rate.\n\nThe most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.\n\nI chose RMSprop (with default values), it is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. We could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop.\n\nA metric is a function that is used to judge the performance of a model. \n\nAccuracy is the fraction of predictions that our model got right = Number of correct predictions / Total number of predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model: \n\ndeep_RU_model.compile(loss=\"categorical_crossentropy\", optimizer = optimizer,metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Annealing method of the learning rate\n**\n\nIn order to make the optimizer converge faster and closest to the global minimum of the loss function, we can use an annealing method of the learning rate (LR).\n\nThe LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n\nIts better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n\nTo keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n\nWith the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 3 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Early stopping and Model Chekpoint\n\nReference:\nhttps://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n\nToo many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. **Early stopping ** is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.\n\nKeras supports the early stopping of training via a callback called EarlyStopping.\n\nThis callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n\nOften, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better.\n\nWe can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the “patience” argument.\n\nThe EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset.\n\nAn additional callback is required that will save the best model observed during training for later use. This is the **ModelCheckpoint** callback.\n\nThe ModelCheckpoint callback is flexible in the way it can be used, but in this case we will use it only to save the best model observed during training as defined by a chosen performance measure on the validation dataset.\n\nSaving and loading models requires that HDF5 support has been installed on your workstation. \nIt may be interesting to know the value of the performance measure and at what epoch the model was saved. This can be printed by the callback by setting the “verbose” argument to “1“."},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=50)\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.6 Fit the model\n\nModel doesn't use all of the data for each step, it only takes some of it. Can be regulated with the batch_size parameter.\nOne time we go throuth the data is called an epoch."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"history = deep_RU_model.fit(datagen.flow(X_train,y_train, batch_size=90), validation_data = (X_val, y_val),\n                            epochs=139, callbacks=[learning_rate_reduction, es, mc])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Model evaluation\n### Training and validation curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the saved model\n# saved_model = load_model('/kaggle/input/deep-ru-letters-cnn-tutorial/best_model.h5')\nsaved_model = load_model('best_model.h5')\n\n# evaluate the model\n_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n_, valid_acc = saved_model.evaluate(X_val, y_val, verbose=0)\n\nprint('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model reaches almost 96% accuracy on the validation dataset after 139 epochs. The validation accuracy is greater than the training accuracy almost evry time during the training. That means that our model dosen't not overfit the training set. The number of epochs could be reduced - there will probably be a slight drop in accuracy (few percent) but the computation won't take that long (~2h15), otherwise GPU could be used to speed it up."},{"metadata":{},"cell_type":"markdown","source":"# 4. Model prediction on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, test_acc = saved_model.evaluate(X_test, y_test, verbose=0)\nprint('Test: %.3f' % (test_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = deep_RU_model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_test = np.argmax(y_test, axis=1)\nconfusion_mtx = confusion_matrix(y_test, y_pred) \nsns.heatmap(confusion_mtx, annot=True, fmt='d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that our CNN performs very well on all letters with few only errors considering the size of the test set.\n\nLet's investigate the most important errors. For that purpose we need to get the difference between the probabilities of a real value and the predicted one."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display some error results \n\n# Convert one-hot vector to labels\nY_true = y_test\n\n# Predict the values from the test dataset\nY_pred = saved_model.predict(X_test)\n# Convert predictions from one-hot vectors to labels\nY_pred_classes = np.argmax(Y_pred,axis = 1) \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_test[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrongly predicted letters\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_delta_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_delta_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During data processing we've shifted letters'order by 1 to make it start from 0. \nThe new ordering is as follows:\n\nа=>0, б=>1, в=>2, г=>3, д=>4, е=>5, ё=>6, ж=>7, з=>8, и=>9,\nй=>10, к=>11, л=>12, м=>13, н=>14, о=>15, п=>16, р=>17, с=>18, т=>19,\nу=>20, ф=>21, х=>22, ц=>23, ч=>24, ш=>25, щ=>26, ъ=>27, ы=>28, ь=>29,\nэ=>30, ю=>31, я=>32\n\nMost of the errors, especially the second row, could be easily made by a human the same way. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThis is a 5 layers Sequential Convolutional Neural Network for Russian handwritten letters recognition. Dataset includs 14190 images with the fowllowing split: \n* 80% train data;\n* 10% validation data;\n* 10% test data.\n\nMy CNN model's architechture: In -> [[Conv2D->relu]* 2 -> MaxPool2D -> Dropout] * 2 -> Flatten -> Dense -> Dropout -> Out\n\nI achieved 95.6% of accuracy with this CNN trained for ~ 2h15 on a single CPU. For those who have a >= 3.0 GPU capabilites (from GTX 650 - to recent GPUs), you can use tensorflow-gpu with keras. Computation will be much much faster!\n\nConverting images to grey scale didn't improve the algorithm.\nI've changed the optimizer from \"adam\" to RMSprop, used an annealing method to find the optimal learning rate, applied several random data augmentation techniques and early stopping to figure out the optimal number of epochs to run the model for. ModelCheckpoint callback is used to save the best model (including the best weights), so it can be used as a pretrained model in the future.  \n\nThe model makes mistakes mostly in those cases where a human eye would've probably guessed wrong as well.\n\nPossible future improvement ideas are the following:\n* try multilabel (add background as a target) classification;\n* improve loading function;\n* remove the backgrounds and then from every sample create many new ones with different backgrounds (as a way of data augmentation);\n* visualize the model to see what else can be improved (how it makes a decision);\n* try monitoring val_loss instead of val_accuracy"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}